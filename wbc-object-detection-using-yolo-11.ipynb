{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10215629,"sourceType":"datasetVersion","datasetId":6314357}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WBC Object Detection\n\nIn this Notebook I have created the model for Detecting th WBCs and it's types and create Bounding Boes around them, Using Yolo11\n\n* **My linkedin id** : [saurabh-maulekhi](https://www.linkedin.com/in/saurabh-maulekhi-326584241/)\n\n* [**YOLO V1 Paper Reimplimentation**](https://www.kaggle.com/code/saurabhmaulekhi/yolo-2017-v1-paper-reimplementation)\n\n* [**Model Web Deployment**](https://huggingface.co/spaces/saurabh091/WBC_Detection_Using_Yolo11)\n\n* [**Github Repo**](https://github.com/Saurabh-Maulekhi/WBCs-s-Object-Detection)\n\n* [**Get my WBCs Object Detection Model**](https://www.kaggle.com/models/saurabhmaulekhi/wbc_detection_with_yolo11)","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"# Install ultralytics\n!pip -q install  ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:44:44.375613Z","iopub.execute_input":"2025-02-20T12:44:44.375893Z","iopub.status.idle":"2025-02-20T12:44:52.185833Z","shell.execute_reply.started":"2025-02-20T12:44:44.375839Z","shell.execute_reply":"2025-02-20T12:44:52.184819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport cv2\nimport yaml\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nimport multiprocessing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:44:52.187009Z","iopub.execute_input":"2025-02-20T12:44:52.187305Z","iopub.status.idle":"2025-02-20T12:45:00.831073Z","shell.execute_reply.started":"2025-02-20T12:44:52.187283Z","shell.execute_reply":"2025-02-20T12:45:00.830421Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Collection and Directories Organisation for training","metadata":{}},{"cell_type":"code","source":"# Path to where your data is stored\nDATA_DIR = Path('/kaggle/input/cytologia-data-challenge')\n\n# Preview data files available\nos.listdir(DATA_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:45:00.832636Z","iopub.execute_input":"2025-02-20T12:45:00.833042Z","iopub.status.idle":"2025-02-20T12:45:00.844850Z","shell.execute_reply.started":"2025-02-20T12:45:00.833019Z","shell.execute_reply":"2025-02-20T12:45:00.844164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up directoris for training a yolo model\n\n# Training,Val,Test directories\nDATASET_DIR = Path('/kaggle/working')\n\nTRAIN_DIR = DATASET_DIR/\"train\"\nVAL_DIR = DATASET_DIR/\"val\"\nTEST_DIR = DATASET_DIR/\"test\"\n\n## images directories\nTRAIN_IMAGES_DIR = TRAIN_DIR/\"images\"\nVAL_IMAGES_DIR = VAL_DIR/\"images\"\nTEST_IMAGES_DIR = TEST_DIR/\"images\"\n\n## label directories\nTRAIN_LABELS_DIR = TRAIN_DIR/\"labels\"\nVAL_LABELS_DIR = VAL_DIR/\"labels\"\nTEST_LABELS_DIR = TEST_DIR/\"labels\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:45:00.846170Z","iopub.execute_input":"2025-02-20T12:45:00.846462Z","iopub.status.idle":"2025-02-20T12:45:00.863939Z","shell.execute_reply.started":"2025-02-20T12:45:00.846440Z","shell.execute_reply":"2025-02-20T12:45:00.863126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creqting Directories\n\nos.mkdir(TRAIN_DIR)\nos.mkdir(VAL_DIR)\nos.mkdir(TEST_DIR)\n\nos.mkdir(TRAIN_IMAGES_DIR)\nos.mkdir(VAL_IMAGES_DIR)\nos.mkdir(TEST_IMAGES_DIR)\n\nos.mkdir(TRAIN_LABELS_DIR)\nos.mkdir(VAL_LABELS_DIR)\nos.mkdir(TEST_LABELS_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:45:00.864754Z","iopub.execute_input":"2025-02-20T12:45:00.865005Z","iopub.status.idle":"2025-02-20T12:45:00.883380Z","shell.execute_reply.started":"2025-02-20T12:45:00.864978Z","shell.execute_reply":"2025-02-20T12:45:00.882628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Overview","metadata":{}},{"cell_type":"code","source":"# Load train and test files\ntrain = pd.read_csv('/kaggle/input/cytologia-data-challenge/cytologia-data.csv')\ntest = pd.read_csv('/kaggle/input/cytologia-data-challenge/cytologia-data Test .csv')\n\n# Add an image_path column\nOG_IMAGES_DIR  = Path(\"/kaggle/input/cytologia-data-challenge/images/dataset_cytologia\")\ntrain['image_path'] = [Path( OG_IMAGES_DIR / x) for x in train.NAME]\ntest['image_path'] = [Path( OG_IMAGES_DIR / x) for x in test.NAME]\n\n# Creating dictionary of key->class_name and value->unique_integer\nclass_list = train[\"class\"].unique()\nclass_len = len(class_list)\nCLASS_DICT = {class_list[x]: x for x in range(0,class_len)}\n\n# Map str classes to ints (label encoding targets)\ntrain['class_id'] = train['class'].map(CLASS_DICT)\n\n# Preview the head of the train set\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:45:00.884213Z","iopub.execute_input":"2025-02-20T12:45:00.884456Z","iopub.status.idle":"2025-02-20T12:45:01.951542Z","shell.execute_reply.started":"2025-02-20T12:45:00.884437Z","shell.execute_reply":"2025-02-20T12:45:01.950617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:45:01.952347Z","iopub.execute_input":"2025-02-20T12:45:01.952582Z","iopub.status.idle":"2025-02-20T12:45:01.960103Z","shell.execute_reply.started":"2025-02-20T12:45:01.952563Z","shell.execute_reply":"2025-02-20T12:45:01.959256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Splitting and Data Transformation","metadata":{}},{"cell_type":"code","source":"# Split data into training and validation\ntrain_unique_imgs_df = train.drop_duplicates(subset = ['NAME'], ignore_index = True)\nX_train, X_val = train_test_split(train_unique_imgs_df, test_size = 0.25, stratify=train_unique_imgs_df['class'], random_state=42)\n\nX_train = train[train.NAME.isin(X_train.NAME)]\nX_val = train[train.NAME.isin(X_val.NAME)]\n\n# Check shapes of training and validation data\nX_train.shape, X_val.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:45:01.962660Z","iopub.execute_input":"2025-02-20T12:45:01.962891Z","iopub.status.idle":"2025-02-20T12:45:02.065956Z","shell.execute_reply.started":"2025-02-20T12:45:01.962871Z","shell.execute_reply":"2025-02-20T12:45:02.065011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:45:02.067436Z","iopub.execute_input":"2025-02-20T12:45:02.067671Z","iopub.status.idle":"2025-02-20T12:45:02.077479Z","shell.execute_reply.started":"2025-02-20T12:45:02.067651Z","shell.execute_reply":"2025-02-20T12:45:02.076624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preview target distribution, seems there a class imbalance that needs to be handled\nX_train['class'].value_counts(normalize = True), X_val['class'].value_counts(normalize = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:45:02.078367Z","iopub.execute_input":"2025-02-20T12:45:02.078624Z","iopub.status.idle":"2025-02-20T12:45:02.106199Z","shell.execute_reply.started":"2025-02-20T12:45:02.078591Z","shell.execute_reply":"2025-02-20T12:45:02.105264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Copying Train, Val, Test images from input to their respective working directories\n\nfor img in tqdm(X_train.image_path.unique()):\n  shutil.copy(img, TRAIN_IMAGES_DIR / img.parts[-1])\n\nfor img in tqdm(X_val.image_path.unique()):\n  shutil.copy(img, VAL_IMAGES_DIR / img.parts[-1])\n\nfor img in tqdm(test.image_path.unique()):\n  shutil.copy(img, TEST_IMAGES_DIR / img.parts[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:45:02.106958Z","iopub.execute_input":"2025-02-20T12:45:02.107142Z","iopub.status.idle":"2025-02-20T12:53:01.172565Z","shell.execute_reply.started":"2025-02-20T12:45:02.107126Z","shell.execute_reply":"2025-02-20T12:53:01.171812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"TRAIN_IMAGES_DIR: \", len(os.listdir(TRAIN_IMAGES_DIR)))\nprint(\"VAL_IMAGES_DIR: \", len(os.listdir(VAL_IMAGES_DIR)))\nprint(\"TEST_IMAGES_DIR: \", len(os.listdir(TEST_IMAGES_DIR)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:53:01.173349Z","iopub.execute_input":"2025-02-20T12:53:01.173637Z","iopub.status.idle":"2025-02-20T12:53:01.211900Z","shell.execute_reply.started":"2025-02-20T12:53:01.173601Z","shell.execute_reply":"2025-02-20T12:53:01.211280Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating yaml file","metadata":{}},{"cell_type":"code","source":"# Function to convert the bboxes to yolo format and save them\ndef save_yolo_annotation(row):\n    image_path, class_id, output_dir = row['image_path'], row['class_id'], row['output_dir']\n\n    img = cv2.imread(image_path)\n    if img is None:\n        raise ValueError(f\"Could not read image from path: {image_path}\")\n\n    height, width, _ = img.shape\n\n    label_file = Path(output_dir) / f\"{Path(image_path).stem}.txt\"\n\n    ymin, xmin, ymax, xmax = row['y1'], row['x1'], row['y2'], row['x2']\n\n    # Normalize the coordinates\n    x_center = (xmin + xmax) / 2 / width\n    y_center = (ymin + ymax) / 2 / height\n    bbox_width = (xmax - xmin) / width\n    bbox_height = (ymax - ymin) / height\n\n    with open(label_file, 'a') as f:\n        f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n\n# Parallelize the annotation saving process\ndef process_dataset(dataframe, output_dir):\n    dataframe['output_dir'] = output_dir\n    with multiprocessing.Pool() as pool: ##to apply the save_yolo_annotation function to each row of the DataFrame in parallel.\n        list(tqdm(pool.imap(save_yolo_annotation, dataframe.to_dict('records')), total=len(dataframe)))\n\n# Save train and validation labels to their respective dirs\nprocess_dataset(X_train, TRAIN_LABELS_DIR)\nprocess_dataset(X_val, VAL_LABELS_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:53:01.212664Z","iopub.execute_input":"2025-02-20T12:53:01.212917Z","iopub.status.idle":"2025-02-20T12:53:54.127155Z","shell.execute_reply.started":"2025-02-20T12:53:01.212897Z","shell.execute_reply":"2025-02-20T12:53:54.126414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label = os.listdir('/kaggle/working/train/labels')[10]\nlabel_file = open('/kaggle/working/train/labels/'+ label)\nlabel_file.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:53:54.128080Z","iopub.execute_input":"2025-02-20T12:53:54.128418Z","iopub.status.idle":"2025-02-20T12:53:54.151112Z","shell.execute_reply.started":"2025-02-20T12:53:54.128384Z","shell.execute_reply":"2025-02-20T12:53:54.150515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:53:54.151866Z","iopub.execute_input":"2025-02-20T12:53:54.152145Z","iopub.status.idle":"2025-02-20T12:53:54.162099Z","shell.execute_reply.started":"2025-02-20T12:53:54.152116Z","shell.execute_reply":"2025-02-20T12:53:54.161504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train images dir, Train labels dir\nTRAIN_IMAGES_DIR, TRAIN_LABELS_DIR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:53:54.162836Z","iopub.execute_input":"2025-02-20T12:53:54.163113Z","iopub.status.idle":"2025-02-20T12:53:54.178526Z","shell.execute_reply.started":"2025-02-20T12:53:54.163079Z","shell.execute_reply":"2025-02-20T12:53:54.177897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a data.yaml file required by yolo\nclass_names = train['class'].unique().tolist()\nnum_classes = len(class_names)\n\ndata_yaml = {\n    'train': str(TRAIN_DIR),\n    'val': str(VAL_DIR),\n    'test': str(TEST_DIR),\n    \n    'nc': num_classes,\n    \n    'names': class_names\n}\n\nyaml_path = 'data.yaml'\nwith open(yaml_path, 'w') as file:\n    yaml.dump(data_yaml, file, default_flow_style=False)\n\n# Preview data yaml file\ndata_yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:53:54.179401Z","iopub.execute_input":"2025-02-20T12:53:54.179673Z","iopub.status.idle":"2025-02-20T12:53:54.202046Z","shell.execute_reply.started":"2025-02-20T12:53:54.179644Z","shell.execute_reply":"2025-02-20T12:53:54.201282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot some images and their bboxes to ensure the conversion was done correctly\ndef load_annotations(label_path):\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    boxes = []\n    for line in lines:\n        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n        boxes.append((class_id, x_center, y_center, width, height))\n    return boxes\n\n# Function to plot an image with its bounding boxes\ndef plot_image_with_boxes(image_path, boxes):\n    # Load the image\n    image = cv2.imread(str(image_path))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Get image dimensions\n    h, w, _ = image.shape\n\n    # Plot the image\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)\n\n    # Plot each bounding box\n    for box in boxes:\n        class_id, x_center, y_center, width, height = box\n        # Convert YOLO format to corner coordinates\n        xmin = int((x_center - width / 2) * w)\n        ymin = int((y_center - height / 2) * h)\n        xmax = int((x_center + width / 2) * w)\n        ymax = int((y_center + height / 2) * h)\n\n        # Draw the bounding box\n        plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                          edgecolor='red', facecolor='none', linewidth=2))\n        plt.text(xmin, ymin - 10, f'Class {int(class_id)}', color='red', fontsize=12, weight='bold')\n\n    plt.axis('off')\n    plt.show()\n\n# Directories for images and labels\nIMAGE_DIR = TRAIN_IMAGES_DIR\nLABEL_DIR = TRAIN_LABELS_DIR\n\n# Plot a few images with their annotations\nfor image_name in os.listdir(TRAIN_IMAGES_DIR)[:3]:\n    image_path = IMAGE_DIR / image_name\n    label_path = LABEL_DIR / (image_name.replace('.jpg', '.txt').replace('.png', '.txt'))\n\n    # if label_path.exists():\n    boxes = load_annotations(label_path)\n    print(f\"Plotting {image_name} with {len(boxes)} bounding boxes.\")\n    plot_image_with_boxes(image_path, boxes)\n    # else:\n        # print(f\"No annotations found for {image_name}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:53:54.202844Z","iopub.execute_input":"2025-02-20T12:53:54.203105Z","iopub.status.idle":"2025-02-20T12:53:55.395301Z","shell.execute_reply.started":"2025-02-20T12:53:54.203069Z","shell.execute_reply":"2025-02-20T12:53:55.394386Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"# Load a yolo pretrained model\nmodel = YOLO('yolo11m.pt')\n\n# Fine tune model to our data\nmodel.train(\n    data='data.yaml',          # Path to the dataset configuration\n    epochs=30,                 # Number of epochs\n    imgsz=640,                # Image size (height, width)\n    batch=8,                   # Batch size\n    device=(0,1),                  # Device to use (0 for the first GPU, 1 for the second GPU)\n    patience=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T12:53:55.396379Z","iopub.execute_input":"2025-02-20T12:53:55.396724Z","iopub.status.idle":"2025-02-20T13:18:07.469793Z","shell.execute_reply.started":"2025-02-20T12:53:55.396692Z","shell.execute_reply":"2025-02-20T13:18:07.468798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing Model","metadata":{}},{"cell_type":"code","source":"## Output Directory list \n\nos.listdir('/kaggle/working/runs/detect/train/weights')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:38:10.356535Z","iopub.execute_input":"2025-02-20T13:38:10.356827Z","iopub.status.idle":"2025-02-20T13:38:10.362331Z","shell.execute_reply.started":"2025-02-20T13:38:10.356805Z","shell.execute_reply":"2025-02-20T13:38:10.361641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validate the model on the validation set\nmodel = YOLO('/kaggle/working/runs/detect/train/weights/best.pt') # model path\nresults = model.val()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:38:14.251436Z","iopub.execute_input":"2025-02-20T13:38:14.251730Z","iopub.status.idle":"2025-02-20T13:44:16.993315Z","shell.execute_reply.started":"2025-02-20T13:38:14.251709Z","shell.execute_reply":"2025-02-20T13:44:16.992317Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predting all test images Detection and Detection csv file","metadata":{}},{"cell_type":"code","source":"# Load the trained YOLO model\nmodel = YOLO('/kaggle/working/runs/detect/train/weights/best.pt')\n\n# Path to the test images directory\ntest_dir_path = TEST_IMAGES_DIR\n\n# Get a list of all image files in the test directory\nimage_files = os.listdir(test_dir_path)\n\n# Initialize an empty list to store the results for all images\nall_data = []\n\n# Iterate through each image in the directory\nfor image_file in tqdm(image_files):\n    # Full path to the image\n    img_path = os.path.join(test_dir_path, image_file)\n\n    # Make predictions on the image\n    results = model(img_path)\n\n    # Extract bounding boxes, confidence scores, and class labels\n    boxes = results[0].boxes.xyxy.tolist()  # Bounding boxes in xyxy format\n    classes = results[0].boxes.cls.tolist()  # Class indices\n    confidences = results[0].boxes.conf.tolist()  # Confidence scores\n    names = results[0].names  # Class names dictionary\n\n    if not boxes:\n        # If no detections, add NEG as the class\n        all_data.append({\n            'Image_ID': image_file,\n            'class': 'NEG',\n            'confidence': 1.0,  # You can set this to a default value\n            'ymin': 0,  # Default value (no detection)\n            'xmin': 0,  # Default value (no detection)\n            'ymax': 0,  # Default value (no detection)\n            'xmax': 0   # Default value (no detection)\n        })\n    else:\n        # Iterate through the results for this image\n        for box, cls, conf in zip(boxes, classes, confidences):\n            x1, y1, x2, y2 = box\n            detected_class = names[int(cls)]  # Get the class name from the names dictionary\n\n            # Add the result to the all_data list\n            all_data.append({\n                'Image_ID': image_file,\n                'class': detected_class,\n                'confidence': conf,\n                'ymin': y1,\n                'xmin': x1,\n                'ymax': y2,\n                'xmax': x2\n            })\n\n# Convert the list to a DataFrame for all images\ndetection = pd.DataFrame(all_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T14:10:12.317997Z","iopub.execute_input":"2025-02-20T14:10:12.318358Z","iopub.status.idle":"2025-02-20T14:10:12.590946Z","shell.execute_reply.started":"2025-02-20T14:10:12.318330Z","shell.execute_reply":"2025-02-20T14:10:12.589672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detection.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:18:07.732151Z","iopub.status.idle":"2025-02-20T13:18:07.732541Z","shell.execute_reply":"2025-02-20T13:18:07.732376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detection['class'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:18:07.733670Z","iopub.status.idle":"2025-02-20T13:18:07.734062Z","shell.execute_reply":"2025-02-20T13:18:07.733888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# D\ndetection.to_csv('Detection.csv', index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:18:07.735019Z","iopub.status.idle":"2025-02-20T13:18:07.735428Z","shell.execute_reply":"2025-02-20T13:18:07.735251Z"}},"outputs":[],"execution_count":null}]}